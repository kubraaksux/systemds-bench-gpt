This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
backends/
  __init__.py
  base.py
  mlx_backend.py
  openai_backend.py
evaluation/
  __init__.py
  perf.py
scripts/
  aggregate.py
  report.py
workloads/
  summarization/
    __init__.py
    config.yaml
    loader.py
    prompt.py
  __init__.py
.gitignore
README.md
requirements.txt
runner.py
runner.svg
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="runner.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 14.1.1 (20251213.1925)
 -->
<!-- Title: G Pages: 1 -->
<svg width="841pt" height="350pt"
 viewBox="0.00 0.00 841.00 350.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 346.22)">
<title>G</title><style>.edge>path:hover{stroke-width:8}</style>
<polygon fill="white" stroke="none" points="-4,4 -4,-346.22 836.83,-346.22 836.83,4 -4,4"/>
<!-- backends -->
<g id="node1" class="node">
<title>backends</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#b65353" stroke="black" cx="34.32" cy="-130.57" rx="34.32" ry="18"/>
<text xml:space="preserve" text-anchor="middle" x="34.32" y="-126.69" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">backends</text>
</g>
<!-- runner_py -->
<g id="node7" class="node">
<title>runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#3c5171" stroke="black" cx="482.32" cy="-18" rx="32.99" ry="18"/>
<text xml:space="preserve" text-anchor="middle" x="482.32" y="-14.12" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">runner.py</text>
</g>
<!-- backends&#45;&gt;runner_py -->
<g id="edge1" class="edge">
<title>backends&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M56.94,-116.89C63.35,-113.82 70.45,-110.88 77.32,-109 120.89,-97.06 464.16,-114.37 482.32,-73"/>
<path fill="none" stroke="black" d="M482.32,-72C485.65,-64.41 486.64,-55.61 486.52,-47.38"/>
<polygon fill="#b65353" stroke="black" points="490.01,-47.18 485.8,-37.46 483.03,-47.68 490.01,-47.18"/>
</g>
<!-- backends_mlx_backend -->
<g id="node2" class="node">
<title>backends_mlx_backend</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#b65353" stroke="black" cx="138.32" cy="-130.57" rx="52.15" ry="21.57"/>
<text xml:space="preserve" text-anchor="middle" x="138.32" y="-132.32" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">backends.</text>
<text xml:space="preserve" text-anchor="middle" x="138.32" y="-121.07" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">mlx_backend</text>
</g>
<!-- backends_mlx_backend&#45;&gt;runner_py -->
<g id="edge2" class="edge">
<title>backends_mlx_backend&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M176.38,-115.44C183.91,-113.02 191.8,-110.73 199.32,-109 261.1,-94.8 456.84,-131.05 482.32,-73"/>
</g>
<!-- backends_openai_backend -->
<g id="node3" class="node">
<title>backends_openai_backend</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#b65353" stroke="black" cx="269.32" cy="-130.57" rx="61.16" ry="21.57"/>
<text xml:space="preserve" text-anchor="middle" x="269.32" y="-132.32" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">backends.</text>
<text xml:space="preserve" text-anchor="middle" x="269.32" y="-121.07" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">openai_backend</text>
</g>
<!-- backends_openai_backend&#45;&gt;runner_py -->
<g id="edge3" class="edge">
<title>backends_openai_backend&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M314.81,-115.92C322.95,-113.56 331.38,-111.17 339.32,-109 402.55,-91.76 455.98,-133.01 482.32,-73"/>
</g>
<!-- evaluation -->
<g id="node4" class="node">
<title>evaluation</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#a2b653" stroke="black" cx="384.32" cy="-130.57" rx="35.65" ry="18"/>
<text xml:space="preserve" text-anchor="middle" x="384.32" y="-126.69" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#000000">evaluation</text>
</g>
<!-- evaluation&#45;&gt;runner_py -->
<g id="edge4" class="edge">
<title>evaluation&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M408.93,-117.29C436.85,-103.08 478.96,-80.65 482.32,-73"/>
</g>
<!-- evaluation_perf -->
<g id="node5" class="node">
<title>evaluation_perf</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#9eb34c" stroke="black" cx="482.32" cy="-130.57" rx="44.72" ry="21.57"/>
<text xml:space="preserve" text-anchor="middle" x="482.32" y="-132.32" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#000000">evaluation.</text>
<text xml:space="preserve" text-anchor="middle" x="482.32" y="-121.07" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#000000">perf</text>
</g>
<!-- evaluation_perf&#45;&gt;runner_py -->
<g id="edge5" class="edge">
<title>evaluation_perf&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M478.39,-108.58C477.27,-97.55 477.46,-84.06 482.32,-73"/>
</g>
<!-- numpy -->
<g id="node6" class="node">
<title>numpy</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#40c073" stroke="black" cx="482.32" cy="-312.7" rx="27" ry="18"/>
<text xml:space="preserve" text-anchor="middle" x="482.32" y="-308.82" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#000000">numpy</text>
</g>
<!-- numpy&#45;&gt;evaluation_perf -->
<g id="edge6" class="edge">
<title>numpy&#45;&gt;evaluation_perf</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M482.32,-294.43C482.32,-264.39 482.32,-202.1 482.32,-163.85"/>
<polygon fill="#40c073" stroke="black" points="485.82,-163.87 482.32,-153.87 478.82,-163.87 485.82,-163.87"/>
</g>
<!-- workloads -->
<g id="node8" class="node">
<title>workloads</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#a253b6" stroke="black" cx="580.32" cy="-130.57" rx="35.65" ry="18"/>
<text xml:space="preserve" text-anchor="middle" x="580.32" y="-126.69" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">workloads</text>
</g>
<!-- workloads&#45;&gt;runner_py -->
<g id="edge7" class="edge">
<title>workloads&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M556.07,-117.08C526.49,-101.64 480.6,-76.91 482.32,-73"/>
</g>
<!-- workloads_summarization -->
<g id="node9" class="node">
<title>workloads_summarization</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#a253b6" stroke="black" cx="690.32" cy="-130.57" rx="56.39" ry="21.57"/>
<text xml:space="preserve" text-anchor="middle" x="690.32" y="-132.32" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">workloads.</text>
<text xml:space="preserve" text-anchor="middle" x="690.32" y="-121.07" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">summarization</text>
</g>
<!-- workloads_summarization&#45;&gt;runner_py -->
<g id="edge8" class="edge">
<title>workloads_summarization&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M648.12,-115.81C640.55,-113.46 632.72,-111.1 625.32,-109 562.27,-91.11 455.98,-133.01 482.32,-73"/>
</g>
<!-- workloads_summarization_loader -->
<g id="node10" class="node">
<title>workloads_summarization_loader</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#a947c2" stroke="black" cx="774.32" cy="-312.7" rx="58.51" ry="29.52"/>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-320.07" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">workloads.</text>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-308.82" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">summarization.</text>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-297.57" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">loader</text>
</g>
<!-- workloads_summarization_loader&#45;&gt;runner_py -->
<g id="edge9" class="edge">
<title>workloads_summarization_loader&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M739.73,-288.43C726.69,-277.59 713.32,-263.48 706.32,-247.18 695.96,-223.07 694.47,-211.55 706.32,-188.13 724.07,-153.06 791.62,-166.87 774.32,-131.57"/>
<path fill="none" stroke="black" d="M774.32,-129.57C768.45,-118.59 766.37,-114.72 755.32,-109 700.99,-80.85 457.73,-129.03 482.32,-73"/>
</g>
<!-- workloads_summarization_prompt -->
<g id="node11" class="node">
<title>workloads_summarization_prompt</title><style>.edge>path:hover{stroke-width:8}</style>
<ellipse fill="#9e4cb3" stroke="black" cx="774.32" cy="-217.66" rx="58.51" ry="29.52"/>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-225.03" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">workloads.</text>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-213.78" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">summarization.</text>
<text xml:space="preserve" text-anchor="middle" x="774.32" y="-202.53" font-family="Helvetica,sans-Serif" font-size="10.00" fill="#ffffff">prompt</text>
</g>
<!-- workloads_summarization_loader&#45;&gt;workloads_summarization_prompt -->
<g id="edge10" class="edge">
<title>workloads_summarization_loader&#45;&gt;workloads_summarization_prompt</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M774.32,-282.77C774.32,-275.24 774.32,-267 774.32,-259"/>
<polygon fill="#a947c2" stroke="black" points="777.82,-259.08 774.32,-249.08 770.82,-259.08 777.82,-259.08"/>
</g>
<!-- workloads_summarization_prompt&#45;&gt;runner_py -->
<g id="edge11" class="edge">
<title>workloads_summarization_prompt&#45;&gt;runner_py</title><style>.edge>path:hover{stroke-width:8}</style>
<path fill="none" stroke="black" d="M780.41,-188.04C782.63,-170.82 782.83,-148.94 774.32,-131.57"/>
</g>
</g>
</svg>
</file>

<file path="backends/__init__.py">

</file>

<file path="backends/base.py">
from typing import Any, Dict, List, Optional, Protocol, TypedDict


class GenerationResult(TypedDict, total=False):
    text: str
    latency_ms: float
    tokens: Optional[int]
    extra: Dict[str, Any]


class InferenceBackend(Protocol):
    """
    Minimal contract all inference backends must implement.
    Keep this interface stable after PR1.
    """

    def generate(
        self,
        prompts: List[str],
        config: Dict[str, Any],
    ) -> List[GenerationResult]:
        ...
</file>

<file path="backends/openai_backend.py">
import os
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from openai import OpenAI



class OpenAIBackend:
    """
    Uses the OpenAI Responses API by default (recommended for new projects).
    Stores latency and, when available, usage/cost-related fields in `extra`.
    """

    def __init__(self, api_key: str | None = None):
        load_dotenv()  # loads .env from repo root
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is not set.")
        self.client = OpenAI(api_key=api_key)

    def generate(self, prompts: List[str], config: Dict[str, Any]):
        model = config.get("model", "gpt-4.1-mini")  # safe default; override via config
        max_output_tokens = int(config.get("max_output_tokens", 256))
        # For benchmarking, keep temperature deterministic unless you explicitly vary it.
        temperature = config.get("temperature", 0.0)

        # Simple retry/backoff for transient errors / rate limits
        max_retries = int(config.get("max_retries", 5))
        base_sleep = float(config.get("base_sleep_s", 0.5))

        results = []

        for prompt in prompts:
            last_err = None
            for attempt in range(max_retries):
                try:
                    t0 = time.perf_counter()
                    resp = self.client.responses.create(
                        model=model,
                        input=prompt,
                        max_output_tokens=max_output_tokens,
                        temperature=temperature,
                    )
                    t1 = time.perf_counter()

                    # Extract text output
                    text = ""
                    try:
                        # SDK commonly provides convenience:
                        text = resp.output_text
                    except Exception:
                        # Fallback: be defensive
                        text = str(resp)

                    extra: Dict[str, Any] = {}

                    # Usage fields vary by endpoint/version; keep raw usage if present
                    usage = getattr(resp, "usage", None)
                    if usage is not None:
                        # Make it JSON-serializable
                        if hasattr(usage, "model_dump"):
                            extra["usage"] = usage.model_dump()
                        elif hasattr(usage, "dict"):
                            extra["usage"] = usage.dict()
                        else:
                            extra["usage"] = str(usage)


                    # Also store response id for traceability
                    extra["response_id"] = getattr(resp, "id", None)

                    results.append(
                        {
                            "text": text,
                            "latency_ms": (t1 - t0) * 1000.0,
                            "extra": extra,
                        }
                    )
                    last_err = None
                    break
                except Exception as e:
                    last_err = e
                    time.sleep(base_sleep * (2**attempt))

            if last_err is not None:
                # Fail fast per-sample with explicit error stored
                results.append(
                    {
                        "text": "",
                        "latency_ms": 0.0,
                        "extra": {"error": repr(last_err)},
                    }
                )

        return results
</file>

<file path="evaluation/__init__.py">

</file>

<file path="evaluation/perf.py">
from typing import Dict, List
import numpy as np


def perf_metrics(latencies_ms: List[float], total_wall_s: float) -> Dict[str, float]:
    arr = np.array(latencies_ms, dtype=float)
    if len(arr) == 0:
        return {
            "n": 0.0,
            "latency_ms_mean": 0.0,
            "latency_ms_p50": 0.0,
            "latency_ms_p95": 0.0,
            "throughput_req_per_s": 0.0,
        }

    return {
        "n": float(len(arr)),
        "latency_ms_mean": float(arr.mean()),
        "latency_ms_p50": float(np.percentile(arr, 50)),
        "latency_ms_p95": float(np.percentile(arr, 95)),
        "throughput_req_per_s": float(len(arr) / total_wall_s) if total_wall_s > 0 else 0.0,
    }
</file>

<file path="workloads/summarization/__init__.py">

</file>

<file path="workloads/summarization/loader.py">
from dataclasses import dataclass
from typing import Any, Dict, List


@dataclass
class Sample:
    sid: str
    text: str
    reference: str  # for PR1 we keep a placeholder reference


TOY_TEXTS = [
    "Large language models (LLMs) are widely used in modern applications. They can generate text, summarize documents, and answer questions.",
    "SystemDS is a machine learning system designed for flexible and scalable analytics. It supports declarative ML programming and optimization.",
    "Benchmarking inference systems involves measuring latency, throughput, and quality across tasks and models under controlled conditions.",
    "Speculative decoding is a technique to accelerate autoregressive generation by using a smaller draft model and verifying with a larger model.",
    "Reproducible experiments require fixed seeds, versioned configs, and consistent environments across runs.",
    "A good benchmark suite includes diverse workloads such as summarization, question answering, and reasoning tasks.",
    "Local inference can reduce cost and improve privacy, but may be limited by hardware constraints and model support.",
    "Hosted APIs offer strong model quality and easy scaling, but introduce network latency and variable cost per token.",
    "Throughput is typically measured in requests per second or tokens per second, depending on the benchmark design.",
    "Accuracy for summarization can be approximated with overlap metrics, but human evaluation is often the gold standard.",
]


def load_samples(cfg: Dict[str, Any]) -> List[Sample]:
    dataset = cfg.get("dataset", {})
    source = dataset.get("source", "toy")
    n = int(dataset.get("n_samples", 10))

    if source != "toy":
        raise ValueError("PR1 supports only dataset.source: toy")

    texts = TOY_TEXTS[: max(1, min(n, len(TOY_TEXTS)))]
    samples: List[Sample] = []
    for i, t in enumerate(texts):
        samples.append(Sample(sid=f"toy-{i}", text=t, reference=""))
    return samples
</file>

<file path="workloads/__init__.py">

</file>

<file path="workloads/summarization/config.yaml">
name: summarization

dataset:
  # PR1: keep it fully local and deterministic (no HF download required)
  source: toy
  n_samples: 10

generation:
  max_tokens: 80
  temperature: 0.0

openai:
  model: gpt-4.1-mini
  max_output_tokens: 128
  temperature: 0.0
  max_retries: 5
  base_sleep_s: 0.5
</file>

<file path="workloads/summarization/prompt.py">
from typing import Any, Dict
from .loader import Sample


def make_prompt(sample: Sample, cfg: Dict[str, Any]) -> str:
    return (
        "Summarize the following text in exactly 2-3 sentences. "
        "Do not add any facts that are not present in the original text.\n\n"
        f"{sample.text}\n"
    )
</file>

<file path="requirements.txt">
pyyaml>=6.0
numpy>=1.26
tqdm>=4.66
datasets>=2.20
openai>=1.0.0
python-dotenv>=1.0
mlx-lm>=0.20
</file>

<file path="runner.py">
import argparse
import json
import time
from pathlib import Path
from typing import Any, Dict

import hashlib
import platform
import subprocess
import sys
from datetime import datetime, timezone


import yaml

from evaluation.perf import perf_metrics
from workloads.summarization.loader import load_samples
from workloads.summarization.prompt import make_prompt

def json_safe(x):
    if x is None:
        return None
    if isinstance(x, (str, int, float, bool)):
        return x
    if isinstance(x, dict):
        return {str(k): json_safe(v) for k, v in x.items()}
    if isinstance(x, list):
        return [json_safe(v) for v in x]
    # pydantic-like objects
    if hasattr(x, "model_dump"):
        return json_safe(x.model_dump())
    if hasattr(x, "dict"):
        return json_safe(x.dict())
    return str(x)

def write_manifest(out_dir: Path, workload_path: Path, backend: str, model: str) -> None:
    # git commit hash (best-effort)
    git_commit_hash = None
    try:
        r = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            check=True,
        )
        git_commit_hash = r.stdout.strip()
    except Exception:
        git_commit_hash = None

    # workload config hash
    workload_bytes = workload_path.read_bytes()
    workload_sha256 = hashlib.sha256(workload_bytes).hexdigest()

    manifest = {
        "git_commit_hash": git_commit_hash,
        "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "python_version": sys.version,
        "platform": {
            "os": platform.system(),
            "architecture": platform.machine(),
        },
        "backend": backend,
        "model": model,
        "workload_config_path": str(workload_path.resolve()),
        "workload_config_sha256": workload_sha256,
    }
    write_json(out_dir / "manifest.json", manifest)



def write_json(path: Path, obj: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding="utf-8")


def main():
    parser = argparse.ArgumentParser(description="systemds-bench-gpt runner")
    parser.add_argument("--backend", required=True, choices=["mlx", "openai"])
    parser.add_argument("--workload", required=True)
    parser.add_argument("--model", default="")
    parser.add_argument("--out", required=True)
    args = parser.parse_args()

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    cfg: Dict[str, Any] = yaml.safe_load(Path(args.workload).read_text(encoding="utf-8"))

    if args.backend == "mlx":
        if not args.model:
            raise RuntimeError("--model is required for mlx backend.")
        from backends.mlx_backend import MLXBackend
        backend = MLXBackend(args.model)
        backend_cfg = cfg.get("generation", {})
        backend_model = args.model
    else:
        from backends.openai_backend import OpenAIBackend
        backend = OpenAIBackend()
        backend_cfg = cfg.get("openai", {})
        if args.model:
            backend_cfg = {**backend_cfg, "model": args.model}
        backend_model = backend_cfg.get("model", "unknown")

    samples = load_samples(cfg)
    prompts = [make_prompt(s, cfg) for s in samples]

    t0 = time.perf_counter()
    outputs = backend.generate(prompts, backend_cfg)
    t1 = time.perf_counter()

    run_config = {
        "backend": args.backend,
        "backend_model": backend_model,
        "workload": cfg.get("name", "unknown"),
        "n_samples": len(samples),
        "backend_config": backend_cfg,
    }
    write_json(out_dir / "run_config.json", run_config)

    latencies = []
    with (out_dir / "samples.jsonl").open("w", encoding="utf-8") as f:
        for s, o in zip(samples, outputs):
            lat = float(o.get("latency_ms", 0.0))
            latencies.append(lat)
            rec = {
                "id": s.sid,
                "prediction": o.get("text", ""),
                "reference": s.reference,
                "latency_ms": lat,
                "extra": json_safe(o.get("extra", {})),
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    metrics = perf_metrics(latencies, total_wall_s=(t1 - t0))
    write_json(out_dir / "metrics.json", metrics)
    write_manifest(out_dir, Path(args.workload), args.backend, backend_model)


    print(f"OK: wrote {out_dir}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/aggregate.py">
#!/usr/bin/env python3
import argparse
import csv
import json
import sys
from pathlib import Path
from typing import Any, Dict, Iterable, Optional, Tuple


def read_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def is_run_dir(p: Path) -> bool:
    return p.is_dir() and (p / "metrics.json").exists() and (p / "run_config.json").exists()


def iter_run_dirs(results_dir: Path) -> Iterable[Path]:
    """
    Yields run directories that contain metrics.json and run_config.json.

    Supports:
      results/run_xxx/
      results/<group>/run_xxx/   (one-level nesting)
    Avoids duplicates by tracking resolved paths.
    """
    if not results_dir.exists():
        return

    seen = set()

    # Direct children
    for p in results_dir.iterdir():
        if is_run_dir(p):
            rp = p.resolve()
            if rp not in seen:
                seen.add(rp)
                yield p

    # One-level nesting
    for group in results_dir.iterdir():
        if not group.is_dir():
            continue
        for p in group.iterdir():
            if is_run_dir(p):
                rp = p.resolve()
                if rp not in seen:
                    seen.add(rp)
                    yield p


def manifest_timestamp(run_dir: Path) -> str:
    """
    Returns timestamp_utc string from manifest.json if present; else "".
    Kept as ISO8601 string so CSV stays simple.
    """
    mpath = run_dir / "manifest.json"
    if not mpath.exists():
        return ""
    try:
        m = read_json(mpath)
        ts = m.get("timestamp_utc")
        return "" if ts is None else str(ts)
    except Exception:
        return ""


def token_stats(samples_path: Path) -> Tuple[Optional[int], Optional[float], Optional[int], Optional[int]]:
    """
    Returns:
      (total_tokens, avg_tokens, total_input_tokens, total_output_tokens)
    If not available: (None, None, None, None)
    """
    if not samples_path.exists():
        return (None, None, None, None)

    total_tokens = 0
    total_in = 0
    total_out = 0
    count = 0
    saw_any = False

    try:
        with samples_path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue

                usage = (obj.get("extra") or {}).get("usage") or {}
                tt = usage.get("total_tokens")
                it = usage.get("input_tokens")
                ot = usage.get("output_tokens")

                if tt is None and it is None and ot is None:
                    continue

                saw_any = True
                if tt is not None:
                    total_tokens += int(tt)
                if it is not None:
                    total_in += int(it)
                if ot is not None:
                    total_out += int(ot)

                count += 1
    except Exception:
        return (None, None, None, None)

    if not saw_any or count == 0:
        return (None, None, None, None)

    avg = (total_tokens / count) if total_tokens > 0 else None
    return (
        total_tokens if total_tokens > 0 else None,
        avg,
        total_in if total_in > 0 else None,
        total_out if total_out > 0 else None,
    )


def sort_key(run_dir: Path) -> Tuple[int, str, str]:
    """
    Sort runs chronologically by manifest timestamp if available.
    Missing timestamp => later in ordering and sorted by name.
    """
    ts = manifest_timestamp(run_dir)
    missing = 1 if ts == "" else 0
    return (missing, ts, run_dir.name)


def main() -> int:
    parser = argparse.ArgumentParser(description="Aggregate benchmark runs under results/ into CSV.")
    parser.add_argument("--results-dir", default="results", help="Directory containing run folders (default: results)")
    parser.add_argument("--out", default="-", help="Output CSV path or '-' for stdout (default: '-')")
    args = parser.parse_args()

    results_dir = Path(args.results_dir)
    run_dirs = list(iter_run_dirs(results_dir))
    run_dirs.sort(key=sort_key)

    if not run_dirs:
        print(f"Error: no valid run directories found under {results_dir}/", file=sys.stderr)
        return 1

    header = [
        "run_dir",
        "ts",
        "backend",
        "backend_model",
        "workload",
        "n",
        "latency_ms_mean",
        "latency_ms_p50",
        "latency_ms_p95",
        "throughput_req_per_s",
        "total_tokens",
        "avg_tokens",
        "total_input_tokens",
        "total_output_tokens",
    ]

    if args.out == "-":
        out_f = sys.stdout
        close_after = False
    else:
        out_f = open(args.out, "w", encoding="utf-8", newline="")
        close_after = True

    try:
        writer = csv.writer(out_f)
        writer.writerow(header)

        for run_dir in run_dirs:
            try:
                metrics = read_json(run_dir / "metrics.json")
                cfg = read_json(run_dir / "run_config.json")
                ts = manifest_timestamp(run_dir)
                total, avg, total_in, total_out = token_stats(run_dir / "samples.jsonl")

                row = [
                    run_dir.name,
                    ts,
                    cfg.get("backend", ""),
                    cfg.get("backend_model", ""),
                    cfg.get("workload", ""),
                    metrics.get("n", ""),
                    metrics.get("latency_ms_mean", ""),
                    metrics.get("latency_ms_p50", ""),
                    metrics.get("latency_ms_p95", ""),
                    metrics.get("throughput_req_per_s", ""),
                    "" if total is None else total,
                    "" if avg is None else f"{avg:.4f}",
                    "" if total_in is None else total_in,
                    "" if total_out is None else total_out,
                ]
                writer.writerow(row)
            except Exception as e:
                print(f"Warning: skipping {run_dir.name}: {e}", file=sys.stderr)
                continue
    finally:
        if close_after:
            out_f.close()

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
</file>

<file path=".gitignore">
.venv/
__pycache__/
*.pyc
results/
.env
.DS_Store
results_summary.csv
report.html
</file>

<file path="backends/mlx_backend.py">
import time
from typing import Any, Dict, List

import mlx.core as mx
from mlx_lm import load, generate


def greedy_sampler(logits: mx.array) -> mx.array:
    # Pick argmax token (deterministic decoding)
    return mx.argmax(logits, axis=-1)


class MLXBackend:
    def __init__(self, model: str):
        # Fail fast if model load fails (clear error message)
        try:
            self.model, self.tokenizer = load(model)
        except Exception as e:
            raise RuntimeError(f"Failed to load MLX model '{model}': {e!r}") from e

    def generate(self, prompts: List[str], config: Dict[str, Any]):
        max_tokens = int(config.get("max_tokens", 128))

        results = []
        for p in prompts:
            try:
                t0 = time.perf_counter()
                out = generate(
                    self.model,
                    self.tokenizer,
                    p,
                    max_tokens=max_tokens,
                    sampler=greedy_sampler,
                )
                t1 = time.perf_counter()
                
                in_tokens = None
                out_tokens = None

                try:
                    in_tokens = len(self.tokenizer.encode(p))
                    out_tokens = len(self.tokenizer.encode(out))
                except Exception:
                    # Tokenization failure should not break the benchmark
                    pass

                usage = {}
                if in_tokens is not None:
                    usage["input_tokens"] = in_tokens
                if out_tokens is not None:
                    usage["output_tokens"] = out_tokens
                if in_tokens is not None and out_tokens is not None:
                    usage["total_tokens"] = in_tokens + out_tokens

                extra = {"usage": usage} if usage else {}
                results.append(
                    {
                        "text": out,
                        "latency_ms": (t1 - t0) * 1000.0,
                        "extra": extra,
                    }
                )
                
            except Exception as e:
                # Per-sample failure: keep benchmark running
                results.append(
                    {"text": "", "latency_ms": 0.0, "extra": {"error": repr(e)}}
                )
        return results
</file>

<file path="scripts/report.py">
#!/usr/bin/env python3
import argparse
import html
import json
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


def read_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def is_run_dir(p: Path) -> bool:
    return p.is_dir() and (p / "metrics.json").exists() and (p / "run_config.json").exists()


def iter_run_dirs(results_dir: Path) -> List[Path]:
    if not results_dir.exists():
        return []

    seen = set()
    runs: List[Path] = []

    # direct children
    for p in results_dir.iterdir():
        if is_run_dir(p):
            rp = p.resolve()
            if rp not in seen:
                seen.add(rp)
                runs.append(p)

    # one-level nesting
    for group in results_dir.iterdir():
        if not group.is_dir():
            continue
        for p in group.iterdir():
            if is_run_dir(p):
                rp = p.resolve()
                if rp not in seen:
                    seen.add(rp)
                    runs.append(p)

    return runs


def manifest_timestamp(run_dir: Path) -> str:
    mpath = run_dir / "manifest.json"
    if not mpath.exists():
        return ""
    try:
        m = read_json(mpath)
        ts = m.get("timestamp_utc")
        return "" if ts is None else str(ts)
    except Exception:
        return ""


def token_stats(samples_path: Path) -> Tuple[Optional[int], Optional[float], Optional[int], Optional[int]]:
    if not samples_path.exists():
        return (None, None, None, None)

    total_tokens = 0
    total_in = 0
    total_out = 0
    count = 0
    saw_any = False

    try:
        with samples_path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue

                usage = (obj.get("extra") or {}).get("usage") or {}
                tt = usage.get("total_tokens")
                it = usage.get("input_tokens")
                ot = usage.get("output_tokens")

                if tt is None and it is None and ot is None:
                    continue

                saw_any = True
                if tt is not None:
                    total_tokens += int(tt)
                if it is not None:
                    total_in += int(it)
                if ot is not None:
                    total_out += int(ot)

                count += 1
    except Exception:
        return (None, None, None, None)

    if not saw_any or count == 0:
        return (None, None, None, None)

    avg = (total_tokens / count) if total_tokens > 0 else None
    return (
        total_tokens if total_tokens > 0 else None,
        avg,
        total_in if total_in > 0 else None,
        total_out if total_out > 0 else None,
    )


def sort_key(run_dir: Path) -> Tuple[int, str, str]:
    ts = manifest_timestamp(run_dir)
    missing = 1 if ts == "" else 0
    return (missing, ts, run_dir.name)

def ts_sort_value(ts: str) -> str:
    # ISO timestamps sort lexicographically; make missing timestamps very old.
    return ts if ts else "0000-00-00T00:00:00"



def safe_float(x: Any) -> Optional[float]:
    if x is None or x == "":
        return None
    try:
        return float(x)
    except Exception:
        return None


def fmt(x: Any) -> str:
    if x is None:
        return ""
    return html.escape(str(x))


def fmt_num(x: Any, digits: int = 4) -> str:
    v = safe_float(x)
    if v is None:
        return ""
    return f"{v:.{digits}f}"


def main() -> int:
    ap = argparse.ArgumentParser(description="Generate a static HTML report from results/ runs.")
    ap.add_argument("--results-dir", default="results", help="Directory containing run folders (default: results)")
    ap.add_argument("--out", default="report.html", help="Output HTML path (default: report.html)")
    ap.add_argument("--latest", type=int, default=10, help="How many latest runs to highlight (default: 10)")
    args = ap.parse_args()

    results_dir = Path(args.results_dir)
    run_dirs = iter_run_dirs(results_dir)
    run_dirs.sort(key=sort_key)


    if not run_dirs:
        print(f"Error: no valid run directories found under {results_dir}/", file=sys.stderr)
        return 1

    rows: List[Dict[str, Any]] = []
    for run_dir in run_dirs:
        try:
            metrics = read_json(run_dir / "metrics.json")
            cfg = read_json(run_dir / "run_config.json")
            ts = manifest_timestamp(run_dir)
            total, avg, total_in, total_out = token_stats(run_dir / "samples.jsonl")

            rows.append(
                {
                    "run_dir": run_dir.name,
                    "ts": ts,
                    "backend": cfg.get("backend", ""),
                    "backend_model": cfg.get("backend_model", ""),
                    "workload": cfg.get("workload", ""),
                    "n": metrics.get("n", ""),
                    "lat_mean": metrics.get("latency_ms_mean", ""),
                    "lat_p50": metrics.get("latency_ms_p50", ""),
                    "lat_p95": metrics.get("latency_ms_p95", ""),
                    "thr": metrics.get("throughput_req_per_s", ""),
                    "total_tokens": total,
                    "avg_tokens": avg,
                    "total_input_tokens": total_in,
                    "total_output_tokens": total_out,
                }
            )
        except Exception as e:
            print(f"Warning: skipping {run_dir.name}: {e}", file=sys.stderr)


    rows_sorted = sorted(rows, key=lambda r: ts_sort_value(str(r.get("ts", "")))) 
    latest_rows = rows_sorted[-args.latest:] if args.latest > 0 else []


    gen_ts = datetime.now(timezone.utc).isoformat()


    def table_html(title: str, table_rows: List[Dict[str, Any]]) -> str:
        cols = [
            ("run_dir", "Run"),
            ("ts", "Timestamp (UTC)"),
            ("backend", "Backend"),
            ("backend_model", "Model"),
            ("workload", "Workload"),
            ("n", "n"),
            ("lat_mean", "lat mean (ms)"),
            ("lat_p50", "p50 (ms)"),
            ("lat_p95", "p95 (ms)"),
            ("thr", "throughput (req/s)"),
            ("total_tokens", "total tok"),
            ("avg_tokens", "avg tok"),
            ("total_input_tokens", "in tok"),
            ("total_output_tokens", "out tok"),
            ("tokps_total", "tok/s (total)"),
            ("mstok_total", "ms/tok (total)"),
            ("tokps_out", "tok/s (out)"),
            ("mstok_out", "ms/tok (out)"),
        ]

        out: List[str] = [f"<h2>{fmt(title)}</h2>", "<table>", "<thead><tr>"]
        for _, label in cols:
            out.append(f"<th>{fmt(label)}</th>")
        out.append("</tr></thead><tbody>")

        for r in table_rows:
            # Derived normalization metrics (best-effort)
            thr = safe_float(r.get("thr"))
            avg_tok = safe_float(r.get("avg_tokens"))
            n = safe_float(r.get("n"))
            out_total = safe_float(r.get("total_output_tokens"))

            avg_out = (out_total / n) if (out_total is not None and n is not None and n > 0) else None

            tokps_total = (thr * avg_tok) if (thr is not None and avg_tok is not None) else None
            mstok_total = (1000.0 / tokps_total) if (tokps_total is not None and tokps_total > 0) else None

            tokps_out = (thr * avg_out) if (thr is not None and avg_out is not None) else None
            mstok_out = (1000.0 / tokps_out) if (tokps_out is not None and tokps_out > 0) else None

            out.append("<tr>")
            out.append(f"<td>{fmt(r.get('run_dir'))}</td>")
            out.append(f"<td>{fmt(r.get('ts'))}</td>")
            out.append(f"<td>{fmt(r.get('backend'))}</td>")
            out.append(f"<td>{fmt(r.get('backend_model'))}</td>")
            out.append(f"<td>{fmt(r.get('workload'))}</td>")
            out.append(f"<td>{fmt(r.get('n'))}</td>")
            out.append(f"<td>{fmt_num(r.get('lat_mean'), 2)}</td>")
            out.append(f"<td>{fmt_num(r.get('lat_p50'), 2)}</td>")
            out.append(f"<td>{fmt_num(r.get('lat_p95'), 2)}</td>")
            out.append(f"<td>{fmt_num(r.get('thr'), 4)}</td>")
            out.append(f"<td>{fmt(r.get('total_tokens'))}</td>")
            out.append(f"<td>{fmt_num(r.get('avg_tokens'), 2)}</td>")
            out.append(f"<td>{fmt(r.get('total_input_tokens'))}</td>")
            out.append(f"<td>{fmt(r.get('total_output_tokens'))}</td>")
            out.append(f"<td>{fmt_num(tokps_total, 2)}</td>")
            out.append(f"<td>{fmt_num(mstok_total, 2)}</td>")
            out.append(f"<td>{fmt_num(tokps_out, 2)}</td>")
            out.append(f"<td>{fmt_num(mstok_out, 2)}</td>")
            out.append("</tr>")

        out.append("</tbody></table>")
        return "\n".join(out)

    html_doc = f"""<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>systemds-bench-gpt report</title>
  <style>
    body {{ font-family: system-ui, -apple-system, sans-serif; margin: 24px; }}
    h1 {{ margin: 0 0 6px 0; }}
    .meta {{ color: #555; margin-bottom: 18px; }}
    table {{ border-collapse: collapse; width: 100%; margin: 10px 0 24px 0; }}
    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; font-size: 13px; }}
    th {{ background: #f6f6f6; }}
    tr:nth-child(even) {{ background: #fbfbfb; }}
    code {{ background: #f2f2f2; padding: 1px 4px; border-radius: 4px; }}
  </style>
</head>
<body>
  <h1>systemds-bench-gpt report</h1>
  <div class="meta">Generated (UTC): <code>{fmt(gen_ts)}</code> | Runs found: <code>{len(rows)}</code></div>

  {table_html("Latest runs", latest_rows)}
  {table_html("All runs", rows_sorted)}


</body>
</html>
"""

    Path(args.out).write_text(html_doc, encoding="utf-8")
    print(f"OK: wrote {args.out}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
</file>

<file path="README.md">
# SYSTEMDS-BENCH-GPT

Backend-agnostic benchmarking suite for Large Language Model (LLM) inference systems.

SYSTEMDS-BENCH-GPT is a systems-oriented evaluation harness for comparing local LLM inference runtimes and hosted LLM APIs under controlled workloads, with a focus on latency, throughput, token efficiency, and runtime stability rather than leaderboard-style task accuracy.

---

## Scope

This repository implements a benchmarking framework to evaluate:

- local inference engines (e.g. MLX, vLLM, SystemDS)
- hosted LLM APIs (e.g. OpenAI-compatible endpoints)

The goal is to enable fair, reproducible, and extensible inference-system comparisons across backends and workloads.

---

## Supported Backends

Currently implemented:

- Local inference
  - MLX (`mlx-community/*` models)

- Hosted APIs
  - OpenAI-compatible APIs (e.g. `gpt-4.1-mini`)

Planned:
- vLLM backend adapter
- SystemDS inference runtime integration

---

## Repository Structure

- `backends/` — backend adapters (mlx, openai, etc.)
- `workloads/` — workload definitions
- `scripts/` — aggregation and report generation
- `results/` — per-run outputs (generated, ignored by git)
- `runner.py` — experiment runner
- `README.md` — project documentation


---

## Installation

Install Python dependencies:

pip install -r requirements.txt

For hosted APIs, ensure the required API keys are available in the environment (e.g. `OPENAI_API_KEY`).

---

## Running a Benchmark

### Example: local MLX summarization

python -u runner.py
--backend mlx
--workload workloads/summarization/config.yaml
--model mlx-community/Phi-3-mini-4k-instruct-4bit
--out results/run_mlx_$(date +%Y%m%d_%H%M%S)

### Example: OpenAI API summarization

python -u runner.py
--backend openai
--workload workloads/summarization/config.yaml
--model gpt-4.1-mini
--out results/run_openai_$(date +%Y%m%d_%H%M%S)

Each run produces:

- `samples.jsonl` — per-request outputs and metadata
- `metrics.json` — aggregated performance metrics
- `run_config.json` — exact configuration used
- `manifest.json` — timestamp and environment metadata

---

## Aggregating Results

Aggregate all completed runs into a single CSV summary:
python scripts/aggregate.py --out results_summary.csv

The aggregated output includes:
- mean, p50, and p95 latency
- throughput (requests/sec)
- input, output, and total token counts
- normalized token-based metrics when available

---

## Generating the HTML Report

Generate a static HTML report for inspection and sharing:
python scripts/report.py
open report.html

The report includes:
- latest runs (by timestamp)
- full run history
- derived normalization metrics such as tokens/sec and ms/token

The HTML report is a generated artifact and is not tracked in version control.

---

## Metrics Reported

For each run, the benchmark reports:

**Latency**
- mean latency
- p50 latency
- p95 latency

**Throughput**
- requests per second (req/s)

**Token accounting**
- input tokens
- output tokens
- total tokens
- average tokens per request

**Derived normalization**
- tokens/sec (total and output)
- ms/token (total and output)

These metrics allow fair comparison across backends and models.

---

## Current Limitations

This benchmark intentionally prioritizes performance and systems behavior over full task quality.

Known limitations:

- no accuracy metrics yet (e.g. ROUGE, F1)
- only summarization workload implemented
- no concurrency or batching sweeps
- TTFT (time-to-first-token) not measured
- no cost modeling for hosted APIs
- tokenization parity across backends is best-effort

Limitations are explicitly documented to avoid overclaiming.

---

## Intended Use

This benchmark is intended for:

- systems research and evaluation
- inference runtime comparison
- performance profiling under controlled workloads
- internal benchmarking and regression tracking

It is not intended as:

- a leaderboard
- a prompt engineering evaluation
- a replacement for task-quality benchmarks

---

## Resources

- **Purpose**: Systems-oriented benchmarking of LLM inference backends
- **Focus**: Latency, throughput, token efficiency, and runtime stability
- **Audience**: Systems researchers, inference engineers, and ML infrastructure developers
- **Status**: Active research prototype

---

## Roadmap

Planned extensions include:

- additional workloads (extraction / JSON, embeddings)
- lightweight accuracy evaluation per workload
- TTFT and decoding-time separation
- concurrency and batching sweeps
- cost-aware metrics for hosted APIs
- additional backend adapters (vLLM, SystemDS)

---

## License

This project is intended for research and educational use.  
License information will be added as the project matures.
</file>

</files>
