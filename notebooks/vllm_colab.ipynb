{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Benchmarking for systemds-bench-gpt\n",
    "\n",
    "This notebook runs vLLM benchmarks on Google Colab's GPU.\n",
    "\n",
    "**After running this notebook:**\n",
    "1. Download `vllm_results.zip`\n",
    "2. Unzip it in your project folder\n",
    "3. Run `python scripts/report.py --out benchmark_report.html`\n",
    "4. The report will include vLLM alongside OpenAI, MLX, Ollama\n",
    "\n",
    "**Requirements:** Enable GPU runtime (Runtime → Change runtime type → T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check GPU is available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install dependencies\n",
    "!pip install vllm torch transformers accelerate -q\n",
    "!pip install pyyaml numpy tqdm datasets requests psutil rouge-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Upload your project files\n",
    "# Option A: Clone from GitHub (update URL to your repo)\n",
    "# !git clone https://github.com/YOUR_USERNAME/systemds-bench-gpt.git\n",
    "# %cd systemds-bench-gpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Start vLLM server\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "MODEL = \"microsoft/phi-2\"  # 2.7B params, fits in T4 GPU\n",
    "\n",
    "print(f\"Starting vLLM server with model: {MODEL}\")\n",
    "print(\"This takes ~2 minutes to load the model...\")\n",
    "\n",
    "server_process = subprocess.Popen(\n",
    "    [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "     \"--model\", MODEL,\n",
    "     \"--host\", \"0.0.0.0\",\n",
    "     \"--port\", \"8000\",\n",
    "     \"--dtype\", \"float16\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "time.sleep(120)  # Wait for model to load\n",
    "print(\"Server ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Verify server is running\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    resp = requests.get(\"http://localhost:8000/v1/models\", timeout=10)\n",
    "    print(\"✓ vLLM server is running!\")\n",
    "    print(resp.json())\n",
    "except Exception as e:\n",
    "    print(f\"✗ Server not ready: {e}\")\n",
    "    print(\"Re-run the previous cell and wait longer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run ALL benchmarks\n",
    "# Results will be saved in the same format as OpenAI/MLX/Ollama\n",
    "\n",
    "workloads = [\"math\", \"reasoning\", \"summarization\", \"json_extraction\"]\n",
    "\n",
    "for wl in workloads:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running {wl} benchmark...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    !python runner.py \\\n",
    "        --backend vllm \\\n",
    "        --model microsoft/phi-2 \\\n",
    "        --workload workloads/{wl}/config.yaml \\\n",
    "        --out results/vllm_{wl}_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: View results summary\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"vLLM BENCHMARK RESULTS (microsoft/phi-2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for run_dir in sorted(os.listdir(\"results\")):\n",
    "    if run_dir.startswith(\"vllm_\"):\n",
    "        metrics_path = f\"results/{run_dir}/metrics.json\"\n",
    "        if os.path.exists(metrics_path):\n",
    "            with open(metrics_path) as f:\n",
    "                m = json.load(f)\n",
    "            workload = run_dir.replace(\"vllm_\", \"\").replace(\"_colab\", \"\")\n",
    "            print(f\"\\n{workload.upper()}:\")\n",
    "            print(f\"  Accuracy:   {m.get('accuracy_count', 'N/A')} ({m.get('accuracy_mean', 0)*100:.0f}%)\")\n",
    "            print(f\"  Latency:    {m.get('latency_ms_p50', 0):.0f}ms (p50)\")\n",
    "            print(f\"  Throughput: {m.get('throughput_req_per_s', 0):.2f} req/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Download vLLM results\n",
    "# Unzip this in your project's results/ folder, then regenerate report\n",
    "\n",
    "!mkdir -p vllm_only_results\n",
    "!cp -r results/vllm_* vllm_only_results/\n",
    "!zip -r vllm_results.zip vllm_only_results/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('vllm_results.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Download vllm_results.zip (should auto-download)\")\n",
    "print(\"2. Unzip into your project: unzip vllm_results.zip -d results/\")\n",
    "print(\"3. Regenerate report: python scripts/report.py --out benchmark_report.html\")\n",
    "print(\"4. Open benchmark_report.html - vLLM will appear with other backends!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Cleanup - stop the server\n",
    "server_process.terminate()\n",
    "print(\"vLLM server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
