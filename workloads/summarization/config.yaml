name: summarization

dataset:
  # PR1: keep it fully local and deterministic (no HF download required)
  source: toy
  n_samples: 10

generation:
  # Used by local MLX backend (you override model via CLI)
  max_tokens: 128
  temperature: 0.0

openai:
  model: gpt-4.1-mini
  max_output_tokens: 128
  temperature: 0.0
  max_retries: 5
  base_sleep_s: 0.5
